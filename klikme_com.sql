-- phpMyAdmin SQL Dump
-- version 4.5.1
-- http://www.phpmyadmin.net
--
-- Host: 127.0.0.1
-- Generation Time: May 13, 2017 at 09:17 PM
-- Server version: 10.1.19-MariaDB
-- PHP Version: 5.6.28

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Database: `klikme.com`
--

-- --------------------------------------------------------

--
-- Table structure for table `attachment`
--

CREATE TABLE `attachment` (
  `attach_id` bigint(100) NOT NULL,
  `name` mediumtext NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

-- --------------------------------------------------------

--
-- Table structure for table `comment`
--

CREATE TABLE `comment` (
  `comment_id` bigint(20) UNSIGNED NOT NULL,
  `reply_id` bigint(20) UNSIGNED NOT NULL,
  `content` longtext NOT NULL,
  `user_id` bigint(20) UNSIGNED NOT NULL,
  `topic_id` bigint(20) UNSIGNED NOT NULL,
  `deleted` tinyint(1) UNSIGNED NOT NULL DEFAULT '0',
  `attach_id` bigint(100) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

-- --------------------------------------------------------

--
-- Table structure for table `follow`
--

CREATE TABLE `follow` (
  `user_id` bigint(20) UNSIGNED NOT NULL,
  `topic_id` bigint(20) UNSIGNED NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

-- --------------------------------------------------------

--
-- Table structure for table `report`
--

CREATE TABLE `report` (
  `report_id` bigint(20) NOT NULL,
  `id_topic` bigint(20) NOT NULL,
  `id_user` bigint(20) NOT NULL,
  `reason` varchar(255) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

--
-- Dumping data for table `report`
--

INSERT INTO `report` (`report_id`, `id_topic`, `id_user`, `reason`) VALUES
(1, 7, 0, 'spam'),
(2, 7, 0, 'spam');

-- --------------------------------------------------------

--
-- Table structure for table `topic`
--

CREATE TABLE `topic` (
  `topic_id` bigint(20) UNSIGNED NOT NULL,
  `user_id` bigint(20) NOT NULL,
  `total_views` bigint(20) UNSIGNED NOT NULL,
  `title` varchar(255) NOT NULL,
  `posted` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `content` longtext,
  `category` varchar(32) NOT NULL,
  `category2` varchar(32) DEFAULT NULL,
  `category3` varchar(32) DEFAULT NULL,
  `deleted` tinyint(1) DEFAULT '0',
  `star` bigint(200) NOT NULL,
  `picture` varchar(255) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

--
-- Dumping data for table `topic`
--

INSERT INTO `topic` (`topic_id`, `user_id`, `total_views`, `title`, `posted`, `content`, `category`, `category2`, `category3`, `deleted`, `star`, `picture`) VALUES
(1, 7, 0, 'Has Intel Invented a Universal Memory Tech?', '2017-05-13 16:56:41', 'Today’s computers shuttle data around a byzantine system of several different kinds of short- and long-term memory. No wonder, then, that engineers have long dreamed of one memory technology to rule them all, a universal memory that would simplify computing and streamline the path of data.\r\n\r\nIn March, Intel announced that it will sell to data centers a new kind of solid-state drive, called Optane, that it says could lead to this kind of simplification. Optane drives are nonvolatile, like flash memory, which means that they should use relatively little standby power and that they’re fast, like DRAM. “It really starts to marry the worlds of memory and storage together,” company CEO Brian Krzanich says in a promotional video, over the swells of heroic music. The technology “comes close to being the holy grail of memory,” says Intel executive vice president William Holt in the same video.\r\n\r\nWhether 3D XPoint, the mystery technology inside Optane, can live up to this promise is likely to depend on the performance it delivers as well as Intel’s ability to scale up manufacturing using new materials and build out the right market. The 375-gigabyte Optane drive on offer now costs US $1,520, about three times the price of an equivalent solid-state drive.\r\n\r\nThis first product will enable data centers to do more with a smaller number of servers, says James Myers, who works on nonvolatile memory architecture at Intel. Myers gives an example of servers running a MySQL database, which, among other things, apps use to store instant messages. For a transaction to be useful in such applications, the data needs to be returned fast—within 10 milliseconds. An equivalent flash drive can perform 1,400 useful transactions per second; the Optane drive can perform over 16,000.\r\n\r\nThe Optane drive was announced with bombast, but the company is coy about the technology behind it. Myers says “3D” refers to the fact that the memory cells are stacked; “XPoint” alludes to the way the memory elements are arranged. While flash memory elements must be read and written in groups, XPoint elements—situated at the crossing point of interconnects—can be addressed individually. Myers says that this architecture, and something inherent to the storage materials themselves, makes 3D XPoint faster than flash memory.\r\n\r\nIntel, which initially developed 3D XPoint in conjunction with Micron, won’t say what the technology really is, but this doesn’t seem to bother researchers or analysts. “Everyone seems to think it’s phase-change memory,” says semiconductor analyst Jim Handy. “I don’t care.” What matters to him—and, Intel hopes, its customers—is the performance.\r\n\r\nThe complexity of today’s memory hierarchy—a combination that often includes magnetic disks and flash for storage and DRAM and static RAM for memory—is a necessary evil. Each technology has its own strengths, so they must be combined. Data are shuttled around from speedy but expensive SRAM caches—which are close to the processor and embedded within it—to slower, less expensive (but still pricey) DRAM. Finally, data are stored in slow but reliable flash or hard-disk drives, or both. Even if it’s not possible to do it all in one memory technology, using only one for working memory close to the processor and one for longer-term storage would help simplify things. Intel says that XPoint memory could provide a speedier alternative to flash memory and magnetic hard disks. The company has also suggested it could supplement or supplant DRAM.\r\n\r\n“DRAM is unique in its ability to waste power, so anything you can do to get rid of it is great,” says Handy. For example, Google is thought to store the index of the entire Internet on several power-hungry, quick-access DRAM servers. If the company could switch this over to 3D XPoint—which Intel claims has 10 times the density of DRAM—Google could use fewer servers and thus save power and money, according to Steven ­Swanson, a computer scientist at the University of California, San Diego.\r\n\r\nIntel is providing software that will enable computers to operate Optane as memory as opposed to storage, but it will be slow. Optane drive latencies max out at 7 or 8 microseconds—way faster than flash, which takes hundreds of microseconds, but not touching DRAM’s low hundreds of nanoseconds. The Optane drives are fettered by the interface they use: They connect to the storage interface, not the memory interface.\r\n\r\nIn the short term, Intel’s drives are not likely to replace any existing memory technologies but will instead supplement them, says Swanson, who built a research drive based on the company’s 3D XPoint technology in 2011. Swanson expects the path to memory and storage simplification to be complex because computing systems will have to be redesigned to route data in new ways.\r\n\r\nSwanson and Handy believe Intel started with storage to help smooth out some of the risk in launching a new memory technology. Making Optane a memory requires new circuit board designs and cooperation from programmers. To get those, Intel needs to show that there is a market for 3D XPoint and demonstrate its reliability. Intel says that a product using a memory interface will be out in 2018. Even using the interface, 3DXPoint still won’t be quite as fast as DRAM, but Intel promises that it will be denser and less expensive.\r\n\r\nThe success of this new memory, then, will hinge on data centers taking it up in a less than ideal initial form while the company works on scaling up production. Even though the flaws in today’s memory and storage hierarchy are universally acknowledged, trying to change it is a risky move. “Almost all memory companies have one or two potential competitors to this technology, and they’re all waiting to see what happens before they jump into [a] big investment,” says Swanson.\r\n\r\nMemory enthusiasts disagree about whether a true universal memory is even physically possible. It is perhaps most useful as a goal to guide the computer industry forward. “The concept of the universal memory is attractive because the idea is to simplify,” says Wei Lu, a computer scientist at the University of Michigan and chief scientist at Crossbar, a resistive RAM startup company. “We have a big-data problem, and today’s computers are fundamentally not good at this.”', 'hardware', NULL, NULL, 0, 9, 'images/1.png'),
(2, 7, 0, 'Google Details Tensor Chip Powers', '2017-05-13 16:58:42', 'Google''s Tensor Processing Unit is a printed-circuit card, which inserts into existing servers and acts as a co-processor, one tailored for neural-network calculations.\r\n\r\nIn January’s special Top Tech 2017 issue, I wrote about various efforts to produce custom hardware tailored for performing deep-learning calculations. Prime among those is Google’s Tensor Processing Unit, or TPU, which Google has deployed in its data centers since early in 2015.\r\n\r\nIn that article, I speculated that the TPU was likely designed for performing what are called  “inference” calculations. That is, it’s designed to quickly and efficiently calculate whatever it is that the neural-network it’s running was created to do. But that neural network would also have to be “trained,” meaning that its many parameters would be tuned to carry out the desired task. Training a neural network normally takes a different set of computational skills: In particular, training often requires the use of higher-precision arithmetic than does inference.\r\n\r\nYesterday, Google released a fairly detailed description of the TPU and its performance relative to CPUs and GPUs. I was happy to see that the surmise I had made in January was correct: The TPU is built for doing inference, having hardware that operates on 8-bit integers rather than higher-precision floating-point numbers.\r\n\r\nYesterday afternoon, David Patterson, an emeritus professor of computer science at the University of California, Berkeley and one of the co-authors of the report, presented these findings at a regional seminar of the National Academy of Engineering, held at the Computer History Museum in Menlo Park, Calif. The abstract for his talk summed up the main point nicely. It reads in part: “The TPU is an order of magnitude faster than contemporary CPUs and GPUs and its relative performance per watt is even larger.”\r\n\r\nGoogle’s blog post about the release of the report shows how much of a difference in relative performance there can be, particularly in regard to energy efficiency. For example, compared with a contemporary GPU, the TPU is said to offer 83 times the performance per watt.  That might be something of an exaggeration, because the report itself claims only that there’s a range of between 41 times and 83 times. And that’s for a quantity the authors call incremental performance. The range of improvement for total performance is less: from 14 to 16 times better for the TPU compared with that of a GPU.\r\n\r\nThe benchmark tests used to reach these conclusions are based on a half dozen of the actual kinds of neural-network programs that people are running at Google data centers. So it’s unlikely that anyone would critique these results on the basis of the tests not reflecting real-world circumstances. But it struck me that a different critique might well be in order.\r\n\r\nThe problem is this: These researchers are comparing their 8-bit TPU with higher-precision GPUs and CPUs, which are just not well suited to inference calculations. The GPU exemplar Google used in its report is Nvidia’s K80 board, which performs both single-precision (32-bit) and double-precision (64-bit) calculations. While they’re often important for training neural networks, such levels of precision aren’t typically needed for inference.\r\n\r\nIn my January story, I noted that Nvidia’s newer Pascal family of GPUs can perform “half-precision” (16-bit) operations and speculated that the company may soon produce units fully capable of 8-bit operations, in which case they might be much more efficient when carrying out inference calculations for neural-network programs.\r\n\r\nThe report’s authors anticipated such a criticism in the final section of their paper; there they considered the assertion (which they label a fallacy) that “CPU and GPU results would be comparable to the TPU if we used them more efficiently or compared to newer versions.” In discussing this point, they say they had tested only one CPU that could support 8-bit calculations, and the TPU was 3.5 times better. But they don’t really address the question of how GPU’s tailored for 8-bit calculations would fare—an important question if such GPUs soon became widely available.\r\n\r\nShould that come to pass, I hope that these Googlers will re-run their benchmarks and let us know how TPUs and 8-bit-capable GPUs compare.', 'hardware', NULL, NULL, 0, 0, 'images/2.png'),
(3, 7, 0, 'Expect Deeper and Cheaper Machine Learning', '2017-05-13 17:01:28', 'Last March, Google’s computers roundly beat the world-class Go champion Lee Sedol, marking a milestone in artificial intelligence. The winning computer program, created by researchers at Google DeepMind in London, used an artificial neural network that took advantage of what’s known as deep learning, a strategy by which neural networks involving many layers of processing are configured in an automated fashion to solve the problem at hand.\r\nUnknown to the public at the time was that Google had an ace up its sleeve. You see, the computers Google used to defeat Sedol contained special-purpose hardware—a computer card Google calls its Tensor Processing Unit.\r\nNorm Jouppi, a hardware engineer at Google, announced the existence of the Tensor Processing Unit two months after the Go match, explaining in a blog post that Google had been outfitting its data centers with these new accelerator cards for more than a year. Google has not shared exactly what is on these boards, but it’s clear that it represents an increasingly popular strategy to speed up deep-learning calculations: using an application-specific integrated circuit, or ASIC.\r\nAnother tactic being pursued (primarily by Microsoft) is to use field-programmable gate arrays (FPGAs), which provide the benefit of being reconfigurable if the computing requirements change. The more common approach, though, has been to use graphics processing units, or GPUs, which can perform many mathematical operations in parallel. The foremost proponent of this approach is GPU maker Nvidia.\r\nIndeed, advances in GPUs kick-started artificial neural networks back in 2009, when researchers at Stanford showed that such hardware made it possible to train deep neural networks in reasonable amounts of time [PDF].\r\n“Everybody is doing deep learning today,” says William Dally, who leads the Concurrent VLSI Architecture group at Stanford and is also chief scientist for Nvidia. And for that, he says, perhaps not surprisingly given his position, “GPUs are close to being as good as you can get.”\r\nDally explains that there are three separate realms to consider. The first is what he calls “training in the data center.” He’s referring to the first step for any deep-learning system: adjusting perhaps many millions of connections between neurons so that the network can carry out its assigned task.\r\nIn building hardware for that, a company called Nervana Systems, which was recently acquired by Intel, has been leading the charge. According to Scott Leishman, a computer scientist at Nervana, the Nervana Engine, an ASIC deep-learning accelerator, will go into production in early to mid-2017. Leishman notes that another computationally intensive task—bitcoin mining—went from being run on CPUs to GPUs to FPGAs and, finally, on ASICs because of the gains in power efficiency from such customization. “I see the same thing happening for deep learning,” he says.\r\nA second and quite distinct job for deep-learning hardware, explains Dally, is “inference at the data center.” The word inference here refers to the ongoing operation of cloud-based artificial neural networks that have previously been trained to carry out some job. Every day, Google’s neural networks are making an astronomical number of such inference calculations to categorize images, translate between languages, and recognize spoken words, for example. Although it’s hard to say for sure, Google’s Tensor Processing Unit is presumably tailored for performing such computations.\r\nTraining and inference often take very different skill sets. Typically for training, the computer must be able to calculate with relatively high precision, often using 32-bit floating-point operations. For inference, precision can be sacrificed in favor of greater speed or less power consumption. “This is an active area of research,” says Leishman. “How low can you go?”\r\nAlthough Dally declines to divulge Nvidia’s specific plans, he points out that the company’s GPUs have been evolving. Nvidia’s earlier Maxwell architecture could perform double- (64-bit) and single- (32-bit) precision operations, whereas its current Pascal architecture adds the capability to do 16-bit operations at twice the throughput and efficiency of its single-precision calculations. So it’s easy to imagine that Nvidia will eventually be releasing GPUs able to perform 8-bit operations, which could be ideal for inference calculations done in the cloud, where power efficiency is critical to keeping costs down.\r\nDally adds that “the final leg of the tripod for deep learning is inference in embedded devices,” such as smartphones, cameras, and tablets. For those applications, the key will be low-power ASICs. Over the coming year, deep-learning software will increasingly find its way into applications for smartphones, where it is already used, for example, to detect malware or translate text in images.\r\nAnd the drone manufacturer DJI is already using something akin to a deep-learning ASIC in its Phantom 4 drone, which uses a special visual-processing chip made by California-based Movidius to recognize obstructions. (Movidius is yet another neural-network company recently acquired by Intel.) Qualcomm, meanwhile, built special circuitry into its Snapdragon 820 processors to help carry out deep-learning calculations.\r\nAlthough there is plenty of incentive these days to design hardware to accelerate the operation of deep neural networks, there’s also a huge risk: If the state of the art shifts far enough, chips designed to run yesterday’s neural nets will be outdated by the time they are manufactured. “The algorithms are changing at an enormous rate,” says Dally. “Everybody who is building these things is trying to cover their bets.”\r\nThis article appears in the January 2017 print issue as “Deeper and Cheaper Machine Learning.”\r\n\r\n', 'hardware', NULL, NULL, 0, 0, 'images/3.png'),
(4, 7, 0, 'AMD Ryzen 16 Core Threadripper CPUs & Whitehaven Platform To Launch On Monstrous 4094 Pin Socket In June', '2017-05-13 17:07:03', 'AMD’s upcoming enthusiast 16 core Ryzen “Threadripper” CPUs and the next generation high-end “Whitehaven” platform will reportedly launch on a monstrous 4094 pin socket shared with AMD’s 32 core Naples parts. The brand new enthusiast Threadripper Ryzen lineup and Whitehaven platform will debut at Computex at the beginning of next month.\r\nAMD Debuting Monstrous 16 Core Threadripper CPUs & SP3r2 Socket For Brand New Enthusiast “Whitehaven” Platform\r\n\r\nThreadripper CPUs will reportedly be compatible with a modified version of AMD’s SP3 server socket designed for its upcoming Naples server parts. The Threadripper SP3r2 socket shares the same physical specifications and pin count of SP3 with some differences in maximum thermal design power and some minor technicalities in PCB implementation. The primary difference between the two being that SP3 sockets will also be available in dual socket server boards, while the SP3r2 socket will only be available on single socket boards.\r\n\r\nUsing the SP3 server socket as a foundation allows AMD to introduce significantly larger chips than Intel’s Skylake X parts as well as what the current mainstream AM4 socket allows. In theory, because the SP3r2 and SP3 sockets are physically the same, AMD has the ability to introduce CPUs with up to 32 cores for its high-end desktop Whitehaven platform although we’re not aware of any such plans.\r\n\r\nUsing the SP3 server socket as a foundation allows AMD to introduce significantly larger chips than Intel’s Skylake X parts as well as what the current mainstream AM4 socket allows. In theory, because the SP3r2 and SP3 sockets are physically the same, AMD has the ability to introduce CPUs with up to 32 cores for its high-end desktop Whitehaven platform although we’re not aware of any such plans.\r\n\r\nThe whitehaven platform offers more I/O, storage, significantly more cores, threads and L3 Cache compared to Intel’s upcoming X299. Ryzen has also established AMD’s Zen architecture as the most power efficient x86 micro architecture to date, putting another arrow in the quiver of Whitehaven’s competitiveness.\r\n\r\nWith a 16 core part in the high-end desktop the Zen architecture is well on its way to displacing Intel’s Extreme edition processors from the performance throne that they’ve long held on to for too long. Threadripper may be the first CPU family from AMD to win the ultimate performance crown since the aptly named “hammer” architecture & Athlon 64 parts hammered Intel’s Extreme Editions nearly 14 years ago.', 'hardware', NULL, NULL, 0, 0, 'images/4.png'),
(5, 7, 0, 'Intel Skylake X Core i9-7920X, Core i9-7900X, Core i9-7820X, Core i9-7800X Mega-Tasking CPUs Leaked – Kaby Lake X Core i7-7740K and Core i7-7640K To Be Entry Level LGA 2066 HEDT Chips', '2017-05-13 17:09:32', 'The latest specifications aren’t confirmed by Intel as of yet but they are very juicy in terms of details. The first most interesting detail is that Intel is finally going to stop using the age-old Core i7 branding on their high-end processors and go straight for Core i9 branding. The new Skylake X processor family is targeted at very high end enthusiasts and multi-tasking geared desktop PCs that require top notch performance in various tasks.\r\n\r\nWhile the Skylake X family will be entirely composed of Core i9 series processors, the Kaby Lake X family will feature the Core i7 branding as they still are processors (although entry level) for the HEDT X299 PCH. There are a total of four Skylake X processors which come with the new 14nm Skylake architecture while the two Kaby Lake X processors features the latest 14nm revision to Skylake, known as the Kaby Lake uArch.\r\n\r\nIntel Core i9-7920X – Intel’s Flagship Skylake X Processor Featuring 12 Cores, 24 Threads\r\n\r\nStarting with the flagship, we have the Core i9-7920X processor. This chip is a juggernaut featuring a total of 12 cores and 24 threads. The total cache on this behemoth is 16.5 MB (L3). Although lower than whole cache featured on previous HEDT processors, the new cache runs more efficiently and reduces chip size and cost while delivering better performance. The chip will ship with a TDP of 160W and will feature a quad channel IMC allowing for up to eight DDR4 DIMMs (two per channel) clocked at 2666 MHz (native).\r\n\r\nThe chip will also feature a total 44 PCIe Gen 3.0 lanes which are enough for running a range of multiple discrete GPUs and fast NVMe storage devices. The chip is expected to launch in August, a whole month after the initial lineup hits the market and will cost close to $1500 US as we have seen already with the 10 Core, 20 thread pricing of Broadwell-E Core i7-6950X flagship CPU.\r\n\r\nIntel Core i9-7900X – Intel’s Massive Multi-Tasking Processor Featuring 10 Cores, 20 Threads\r\n\r\nThe Intel Core i9-7900X is the 10 Core, 20 thread replacement to the Core i7-6950X. This chip will come with 10 cores, 20 threads but on a new Skylake architecture. The chip would feature a total of 13.75 MB cache, that’s 1.375 MB per core. The core clocks are maintained at 3.3 GHz base, 4.3 GHz boost with Intel Turbo 2.0 and 4.5 GHz boost with Intel Turbo 3.0. That’s a vast improvement over the current models and the chip will supposedly feature good overclocking capabilities with its TDP of 160W. The chip will have 44 PCIe Gen 3.0 lanes.\r\n\r\nThe chip will be aimed as a fast multi-threaded option for users who cannot afford the Core i9-7920X. It is expected to launch on June 26th for a price close $999 US or beyond. There’s a chance that Intel may give Skylake X more competitive pricing based on the fact that Ryzen 7 is already out in the market and AMD is soon going to launch their HEDT X399 processors featuring 8, 12, 16 core processors.\r\nIntel Core i9-7820X – Intel’s More Cost Optimized 8 Core, 16 Thread Chip Replacing the Core i7-6900K\r\n\r\nThe Core i9-7820X will be Intel’s cost optimized, under $1000 US variant. It will ship with 8 cores, 16 threads. The chip will be clocked at 3.6 GHz base, 4.3 GHz boost with Turbo 2.0 and 4.5 GHz base with Turbo 3.0. The chip carries 11 MB of L3 cache and will feature 28 PCI-e Gen 3.0 lanes. This processor is going to feature a TDP of 140W.\r\n\r\nThe chip will be a replacement to the Core i7-6900K which has been a very popular chip on Intel’s current X99 enthusiast platform. The chip will also launch in June and will directly tackle the Ryzen 7 1800X. Although, price wise, it could sell for higher versus 1800X’s $499 US market price.\r\nIntel Core i9-7800X – Intel’s Most Affordable 6 Core, 12 Thread Option in The Skylake X Family\r\n\r\nLastly, we have the most affordable Skylake X option of the four. Do note that I say Skylake X and not the entire X299 family as there are also Kaby Lake X chips coming to the X299 platform. The Core i9-7800X is going to feature 6 cores, 12 threads and will carry a total of 8.25 MB of L3 cache. Clock speeds for this chip will be maintained at 3.5 GHz base and 4.0 GHz with Turbo Boost 2.0.\r\n\r\nThere’s no Turbo Boost 3.0 on this chip but Skylake X has one significant advantage on all chips. The L2 cache has been increased from 256 KB per core to 1 MB per core and that will lead to improvements in overall performance of the system. The chip will feature a TDP of 140W and will be available in June. This part will directly tackle the Ryzen 5 1600X with 6 cores and 12 threads but would likely come at a higher price tag since the AMD part costs $249 US and the Core i7-6800K which the Core i9-7800X was close to $400 US in pricing.\r\n\r\nIntel Core i7 7740K Processor Specifications – The Fastest Quad Core Chip\r\n\r\nThe Intel Core i7 7740K processor will become the fastest Core i7 chip in the Kaby Lake lineup. It will replace the Core i7 7700K with slightly better specs. This chip features a quad core, hyper-threaded design. The chip is based on the latest 14nm+ process node which delivers improved efficiency and performance on the existing 14nm FinFET technology.\r\n\r\nThe clock speeds are rated 4.2 GHz base and 4.5 GHz boost. The processor packs 8 MB of L3 cache and has a TDP of 112W. There’s no word on the retail price but it should be a bit higher, or if Intel does feel like offering real competition to the Ryzen chips, they could price it the same as the Core i7 7700K while dropping the former in price.\r\nIntel Core i7 7640K Processor Specifications – The Entry Level, Under $300 US HEDT Chip\r\n\r\nThe Intel Core i7 7640K is also surprisingly now listed as a Core i7 chip rather than Core i5 as previously indicated. This chip features a quad core design along with four threads. The chip is based on the latest 14nm+ process node which delivers improved efficiency and performance on the existing 14nm FinFET technology. The clock speeds are rated at 4.0 GHz base, but boost would be beyond 4 GHz. The processor packs 6 MB of L3 cache. The TDP is rated at 112W, just like the Core i7 processor in the Kaby Lake-X family.\r\n\r\nOther updates will include support for quad channel memory which will be a good bump over dual channel. The new X299 platform will offer more PCIe lanes from the CPU and PCH. Both quad cores will be pitted against entry level 6 core Ryzen chips as current Intel quad cores provide good productivity and gaming performance against the competition.\r\n\r\n	\r\nHardware  ?  Leak  ?  Rumor\r\nIntel Skylake X Core i9-7920X, Core i9-7900X, Core i9-7820X, Core i9-7800X Mega-Tasking CPUs Leaked – Kaby Lake X Core i7-7740K and Core i7-7640K To Be Entry Level LGA 2066 HEDT Chips\r\nAuthor Photo\r\nBy Hassan Mujtaba\r\nMay 12\r\n921Shares\r\nShare Tweet Submit\r\n\r\nIt looks like Intel is going all out with their upcoming HEDT line of processors that are part of the Skylake X and Kaby Lake X family. The latest leak from Videocardz (via Anandtech Forums) alleges that the chips will feature a brand new naming scheme and also details their specifications.\r\nIntel Skylake X Flagship Core i9-7920X Specifications Supposedly Leaked, Family To Feature Core i9-7900X, Core i9-7820X and Core i9-7800X\r\n\r\nThe latest specifications aren’t confirmed by Intel as of yet but they are very juicy in terms of details. The first most interesting detail is that Intel is finally going to stop using the age-old Core i7 branding on their high-end processors and go straight for Core i9 branding. The new Skylake X processor family is targeted at very high end enthusiasts and multi-tasking geared desktop PCs that require top notch performance in various tasks.\r\nRelated\r\nAMD Ryzen 16 Core Threadripper CPUs & Whitehaven Platform To Launch On Monstrous 4094 Pin Socket In June\r\n\r\nWhile the Skylake X family will be entirely composed of Core i9 series processors, the Kaby Lake X family will feature the Core i7 branding as they still are processors (although entry level) for the HEDT X299 PCH. There are a total of four Skylake X processors which come with the new 14nm Skylake architecture while the two Kaby Lake X processors features the latest 14nm revision to Skylake, known as the Kaby Lake uArch.\r\n\r\nThe Skylake X family houses the flagship chip of the lineup, the Core i9-7920X which as the specifications suggest is a beast of a chip. So let’s get on with the specifications of these HEDT Skylake X and entry-level HEDT Kaby Lake X processors.\r\nIntel Core i9-7920X – Intel’s Flagship Skylake X Processor Featuring 12 Cores, 24 Threads\r\n\r\nStarting with the flagship, we have the Core i9-7920X processor. This chip is a juggernaut featuring a total of 12 cores and 24 threads. The total cache on this behemoth is 16.5 MB (L3). Although lower than whole cache featured on previous HEDT processors, the new cache runs more efficiently and reduces chip size and cost while delivering better performance. The chip will ship with a TDP of 160W and will feature a quad channel IMC allowing for up to eight DDR4 DIMMs (two per channel) clocked at 2666 MHz (native).\r\nRelated\r\nWanaCrypt0r 2.0 Ransomware Has Attacked the NHS – Patient Data Scrambled With Other Organizations Targeted as Well\r\n\r\nIntel’s Skylake-X platform promises to introduce the first ever 12-core SKU for the Mainstream/HEDT segment.\r\n\r\nThe chip will also feature a total 44 PCIe Gen 3.0 lanes which are enough for running a range of multiple discrete GPUs and fast NVMe storage devices. The chip is expected to launch in August, a whole month after the initial lineup hits the market and will cost close to $1500 US as we have seen already with the 10 Core, 20 thread pricing of Broadwell-E Core i7-6950X flagship CPU.\r\nIntel Core i9-7900X – Intel’s Massive Multi-Tasking Processor Featuring 10 Cores, 20 Threads\r\n\r\nThe Intel Core i9-7900X is the 10 Core, 20 thread replacement to the Core i7-6950X. This chip will come with 10 cores, 20 threads but on a new Skylake architecture. The chip would feature a total of 13.75 MB cache, that’s 1.375 MB per core. The core clocks are maintained at 3.3 GHz base, 4.3 GHz boost with Intel Turbo 2.0 and 4.5 GHz boost with Intel Turbo 3.0. That’s a vast improvement over the current models and the chip will supposedly feature good overclocking capabilities with its TDP of 160W. The chip will have 44 PCIe Gen 3.0 lanes.\r\n\r\nIntel X299 motherboards will feature very high-end designs.\r\n\r\nThe chip will be aimed as a fast multi-threaded option for users who cannot afford the Core i9-7920X. It is expected to launch on June 26th for a price close $999 US or beyond. There’s a chance that Intel may give Skylake X more competitive pricing based on the fact that Ryzen 7 is already out in the market and AMD is soon going to launch their HEDT X399 processors featuring 8, 12, 16 core processors.\r\nIntel Core i9-7820X – Intel’s More Cost Optimized 8 Core, 16 Thread Chip Replacing the Core i7-6900K\r\n\r\nThe Core i9-7820X will be Intel’s cost optimized, under $1000 US variant. It will ship with 8 cores, 16 threads. The chip will be clocked at 3.6 GHz base, 4.3 GHz boost with Turbo 2.0 and 4.5 GHz base with Turbo 3.0. The chip carries 11 MB of L3 cache and will feature 28 PCI-e Gen 3.0 lanes. This processor is going to feature a TDP of 140W.\r\n\r\nThe chip will be a replacement to the Core i7-6900K which has been a very popular chip on Intel’s current X99 enthusiast platform. The chip will also launch in June and will directly tackle the Ryzen 7 1800X. Although, price wise, it could sell for higher versus 1800X’s $499 US market price.\r\nIntel Core i9-7800X – Intel’s Most Affordable 6 Core, 12 Thread Option in The Skylake X Family\r\n\r\nLastly, we have the most affordable Skylake X option of the four. Do note that I say Skylake X and not the entire X299 family as there are also Kaby Lake X chips coming to the X299 platform. The Core i9-7800X is going to feature 6 cores, 12 threads and will carry a total of 8.25 MB of L3 cache. Clock speeds for this chip will be maintained at 3.5 GHz base and 4.0 GHz with Turbo Boost 2.0.\r\n\r\nThere’s no Turbo Boost 3.0 on this chip but Skylake X has one significant advantage on all chips. The L2 cache has been increased from 256 KB per core to 1 MB per core and that will lead to improvements in overall performance of the system. The chip will feature a TDP of 140W and will be available in June. This part will directly tackle the Ryzen 5 1600X with 6 cores and 12 threads but would likely come at a higher price tag since the AMD part costs $249 US and the Core i7-6800K which the Core i9-7800X was close to $400 US in pricing.\r\nIntel Skylake X Core i9 Processor Family Specifications (Preliminary):\r\nCPU Name	Intel Core i9-7800X	Intel Core i9-7820X	Intel Core i9-7900X	Intel Core i9-7920X\r\nCPU Process	14nm+	14nm+	14nm+	14nm+\r\nCores/Threads	6/12	8/16	10/20	12/24\r\nBase Clock	3.5 GHz	3.6 GHz	3.3 GHz	TBD\r\nBoost Clock	4.0 GHz	4.5 GHz	4.5 GHz	TBD\r\nL3 Cache	8.25 MB	11 MB	13.75 MB	16.5 MB\r\nMemory Support	Quad Channel DDR4-2667	Quad Channel DDR4-2667	Quad Channel DDR4-2667	Quad Channel DDR4-2667\r\nSocket Type	LGA 2066	LGA 2066	LGA 2066	LGA 2066\r\nTDP	140W	140W	160W	160W\r\nPrice	TBD	TBD	TBD	TBD\r\nIntel Core i7 7740K Processor Specifications – The Fastest Quad Core Chip\r\n\r\nThe Intel Core i7 7740K processor will become the fastest Core i7 chip in the Kaby Lake lineup. It will replace the Core i7 7700K with slightly better specs. This chip features a quad core, hyper-threaded design. The chip is based on the latest 14nm+ process node which delivers improved efficiency and performance on the existing 14nm FinFET technology.\r\n\r\nThe clock speeds are rated 4.2 GHz base and 4.5 GHz boost. The processor packs 8 MB of L3 cache and has a TDP of 112W. There’s no word on the retail price but it should be a bit higher, or if Intel does feel like offering real competition to the Ryzen chips, they could price it the same as the Core i7 7700K while dropping the former in price.\r\nIntel Core i7 7640K Processor Specifications – The Entry Level, Under $300 US HEDT Chip\r\n\r\nThe Intel Core i7 7640K is also surprisingly now listed as a Core i7 chip rather than Core i5 as previously indicated. This chip features a quad core design along with four threads. The chip is based on the latest 14nm+ process node which delivers improved efficiency and performance on the existing 14nm FinFET technology. The clock speeds are rated at 4.0 GHz base, but boost would be beyond 4 GHz. The processor packs 6 MB of L3 cache. The TDP is rated at 112W, just like the Core i7 processor in the Kaby Lake-X family.\r\n\r\nOther updates will include support for quad channel memory which will be a good bump over dual channel. The new X299 platform will offer more PCIe lanes from the CPU and PCH. Both quad cores will be pitted against entry level 6 core Ryzen chips as current Intel quad cores provide good productivity and gaming performance against the competition.\r\n\r\nIntel Kaby Lake-X Core i7-7740K and Core i7-7640K Specs:\r\nCPU Name	Intel Core i5-7600K	Intel Core i7-7640K	Intel Core i7-7700K	Intel Core i7-7740K\r\nCPU Process	14nm+	14nm+	14nm+	14nm+\r\nCores/Threads	4/4	4/4	4/8	4/8\r\nBase Clock	3.8 GHz	4.0 GHz	4.2 GHz	4.2 GHz\r\nBoost Clock	4.2 GHz	4.0 GHz+	4.5 GHz	4.5 GHz\r\nL3 Cache	6 MB L3	6 MB L3	8 MB L3	8 MB L3\r\nMemory Support	DDR4 Dual Channel	DDR4 Dual Channel	DDR4 Dual Channel	DDR4 Dual Channel\r\nSocket Type	LGA 1151	LGA 2066	LGA 1151	LGA 2066\r\nTDP	91W	112W	91W	112W\r\nPrice	$242 US	TBD	$339 US	TBD\r\n\r\nIntel Skylake X Processor Family For Intel X299 Platform Details\r\n\r\nThe Skylake X family in particular will feature five SKUs that include a 12 core, 10 core, 8 core and a 6 core model. The 10, 8 and 6 core models will be based on the Skylake architecture. All Skylake-X chips will feature a rated TDP of 140W and we can expect clock speeds north of the 3.0 GHz range and boosts of 4.0 GHz+ on the 6 core and 8 core chips. The 12 core chip will be the new flagship and will come with more PCIe lanes with faster memory support.\r\n\r\n	\r\nHardware  ?  Leak  ?  Rumor\r\nIntel Skylake X Core i9-7920X, Core i9-7900X, Core i9-7820X, Core i9-7800X Mega-Tasking CPUs Leaked – Kaby Lake X Core i7-7740K and Core i7-7640K To Be Entry Level LGA 2066 HEDT Chips\r\nAuthor Photo\r\nBy Hassan Mujtaba\r\nMay 12\r\n921Shares\r\nShare Tweet Submit\r\n\r\nIt looks like Intel is going all out with their upcoming HEDT line of processors that are part of the Skylake X and Kaby Lake X family. The latest leak from Videocardz (via Anandtech Forums) alleges that the chips will feature a brand new naming scheme and also details their specifications.\r\nIntel Skylake X Flagship Core i9-7920X Specifications Supposedly Leaked, Family To Feature Core i9-7900X, Core i9-7820X and Core i9-7800X\r\n\r\nThe latest specifications aren’t confirmed by Intel as of yet but they are very juicy in terms of details. The first most interesting detail is that Intel is finally going to stop using the age-old Core i7 branding on their high-end processors and go straight for Core i9 branding. The new Skylake X processor family is targeted at very high end enthusiasts and multi-tasking geared desktop PCs that require top notch performance in various tasks.\r\nRelated\r\nAMD Ryzen 16 Core Threadripper CPUs & Whitehaven Platform To Launch On Monstrous 4094 Pin Socket In June\r\n\r\nWhile the Skylake X family will be entirely composed of Core i9 series processors, the Kaby Lake X family will feature the Core i7 branding as they still are processors (although entry level) for the HEDT X299 PCH. There are a total of four Skylake X processors which come with the new 14nm Skylake architecture while the two Kaby Lake X processors features the latest 14nm revision to Skylake, known as the Kaby Lake uArch.\r\n\r\nThe Skylake X family houses the flagship chip of the lineup, the Core i9-7920X which as the specifications suggest is a beast of a chip. So let’s get on with the specifications of these HEDT Skylake X and entry-level HEDT Kaby Lake X processors.\r\nIntel Core i9-7920X – Intel’s Flagship Skylake X Processor Featuring 12 Cores, 24 Threads\r\n\r\nStarting with the flagship, we have the Core i9-7920X processor. This chip is a juggernaut featuring a total of 12 cores and 24 threads. The total cache on this behemoth is 16.5 MB (L3). Although lower than whole cache featured on previous HEDT processors, the new cache runs more efficiently and reduces chip size and cost while delivering better performance. The chip will ship with a TDP of 160W and will feature a quad channel IMC allowing for up to eight DDR4 DIMMs (two per channel) clocked at 2666 MHz (native).\r\nRelated\r\nWanaCrypt0r 2.0 Ransomware Has Attacked the NHS – Patient Data Scrambled With Other Organizations Targeted as Well\r\n\r\nIntel’s Skylake-X platform promises to introduce the first ever 12-core SKU for the Mainstream/HEDT segment.\r\n\r\nThe chip will also feature a total 44 PCIe Gen 3.0 lanes which are enough for running a range of multiple discrete GPUs and fast NVMe storage devices. The chip is expected to launch in August, a whole month after the initial lineup hits the market and will cost close to $1500 US as we have seen already with the 10 Core, 20 thread pricing of Broadwell-E Core i7-6950X flagship CPU.\r\nIntel Core i9-7900X – Intel’s Massive Multi-Tasking Processor Featuring 10 Cores, 20 Threads\r\n\r\nThe Intel Core i9-7900X is the 10 Core, 20 thread replacement to the Core i7-6950X. This chip will come with 10 cores, 20 threads but on a new Skylake architecture. The chip would feature a total of 13.75 MB cache, that’s 1.375 MB per core. The core clocks are maintained at 3.3 GHz base, 4.3 GHz boost with Intel Turbo 2.0 and 4.5 GHz boost with Intel Turbo 3.0. That’s a vast improvement over the current models and the chip will supposedly feature good overclocking capabilities with its TDP of 160W. The chip will have 44 PCIe Gen 3.0 lanes.\r\n\r\nIntel X299 motherboards will feature very high-end designs.\r\n\r\nThe chip will be aimed as a fast multi-threaded option for users who cannot afford the Core i9-7920X. It is expected to launch on June 26th for a price close $999 US or beyond. There’s a chance that Intel may give Skylake X more competitive pricing based on the fact that Ryzen 7 is already out in the market and AMD is soon going to launch their HEDT X399 processors featuring 8, 12, 16 core processors.\r\nIntel Core i9-7820X – Intel’s More Cost Optimized 8 Core, 16 Thread Chip Replacing the Core i7-6900K\r\n\r\nThe Core i9-7820X will be Intel’s cost optimized, under $1000 US variant. It will ship with 8 cores, 16 threads. The chip will be clocked at 3.6 GHz base, 4.3 GHz boost with Turbo 2.0 and 4.5 GHz base with Turbo 3.0. The chip carries 11 MB of L3 cache and will feature 28 PCI-e Gen 3.0 lanes. This processor is going to feature a TDP of 140W.\r\n\r\nThe chip will be a replacement to the Core i7-6900K which has been a very popular chip on Intel’s current X99 enthusiast platform. The chip will also launch in June and will directly tackle the Ryzen 7 1800X. Although, price wise, it could sell for higher versus 1800X’s $499 US market price.\r\nIntel Core i9-7800X – Intel’s Most Affordable 6 Core, 12 Thread Option in The Skylake X Family\r\n\r\nLastly, we have the most affordable Skylake X option of the four. Do note that I say Skylake X and not the entire X299 family as there are also Kaby Lake X chips coming to the X299 platform. The Core i9-7800X is going to feature 6 cores, 12 threads and will carry a total of 8.25 MB of L3 cache. Clock speeds for this chip will be maintained at 3.5 GHz base and 4.0 GHz with Turbo Boost 2.0.\r\n\r\nThere’s no Turbo Boost 3.0 on this chip but Skylake X has one significant advantage on all chips. The L2 cache has been increased from 256 KB per core to 1 MB per core and that will lead to improvements in overall performance of the system. The chip will feature a TDP of 140W and will be available in June. This part will directly tackle the Ryzen 5 1600X with 6 cores and 12 threads but would likely come at a higher price tag since the AMD part costs $249 US and the Core i7-6800K which the Core i9-7800X was close to $400 US in pricing.\r\nIntel Skylake X Core i9 Processor Family Specifications (Preliminary):\r\nCPU Name	Intel Core i9-7800X	Intel Core i9-7820X	Intel Core i9-7900X	Intel Core i9-7920X\r\nCPU Process	14nm+	14nm+	14nm+	14nm+\r\nCores/Threads	6/12	8/16	10/20	12/24\r\nBase Clock	3.5 GHz	3.6 GHz	3.3 GHz	TBD\r\nBoost Clock	4.0 GHz	4.5 GHz	4.5 GHz	TBD\r\nL3 Cache	8.25 MB	11 MB	13.75 MB	16.5 MB\r\nMemory Support	Quad Channel DDR4-2667	Quad Channel DDR4-2667	Quad Channel DDR4-2667	Quad Channel DDR4-2667\r\nSocket Type	LGA 2066	LGA 2066	LGA 2066	LGA 2066\r\nTDP	140W	140W	160W	160W\r\nPrice	TBD	TBD	TBD	TBD\r\nIntel Core i7 7740K Processor Specifications – The Fastest Quad Core Chip\r\n\r\nThe Intel Core i7 7740K processor will become the fastest Core i7 chip in the Kaby Lake lineup. It will replace the Core i7 7700K with slightly better specs. This chip features a quad core, hyper-threaded design. The chip is based on the latest 14nm+ process node which delivers improved efficiency and performance on the existing 14nm FinFET technology.\r\n\r\nThe clock speeds are rated 4.2 GHz base and 4.5 GHz boost. The processor packs 8 MB of L3 cache and has a TDP of 112W. There’s no word on the retail price but it should be a bit higher, or if Intel does feel like offering real competition to the Ryzen chips, they could price it the same as the Core i7 7700K while dropping the former in price.\r\nIntel Core i7 7640K Processor Specifications – The Entry Level, Under $300 US HEDT Chip\r\n\r\nThe Intel Core i7 7640K is also surprisingly now listed as a Core i7 chip rather than Core i5 as previously indicated. This chip features a quad core design along with four threads. The chip is based on the latest 14nm+ process node which delivers improved efficiency and performance on the existing 14nm FinFET technology. The clock speeds are rated at 4.0 GHz base, but boost would be beyond 4 GHz. The processor packs 6 MB of L3 cache. The TDP is rated at 112W, just like the Core i7 processor in the Kaby Lake-X family.\r\n\r\nOther updates will include support for quad channel memory which will be a good bump over dual channel. The new X299 platform will offer more PCIe lanes from the CPU and PCH. Both quad cores will be pitted against entry level 6 core Ryzen chips as current Intel quad cores provide good productivity and gaming performance against the competition.\r\n\r\nIntel Kaby Lake-X Core i7-7740K and Core i7-7640K Specs:\r\nCPU Name	Intel Core i5-7600K	Intel Core i7-7640K	Intel Core i7-7700K	Intel Core i7-7740K\r\nCPU Process	14nm+	14nm+	14nm+	14nm+\r\nCores/Threads	4/4	4/4	4/8	4/8\r\nBase Clock	3.8 GHz	4.0 GHz	4.2 GHz	4.2 GHz\r\nBoost Clock	4.2 GHz	4.0 GHz+	4.5 GHz	4.5 GHz\r\nL3 Cache	6 MB L3	6 MB L3	8 MB L3	8 MB L3\r\nMemory Support	DDR4 Dual Channel	DDR4 Dual Channel	DDR4 Dual Channel	DDR4 Dual Channel\r\nSocket Type	LGA 1151	LGA 2066	LGA 1151	LGA 2066\r\nTDP	91W	112W	91W	112W\r\nPrice	$242 US	TBD	$339 US	TBD\r\n\r\nIntel Skylake X Processor Family For Intel X299 Platform Details\r\n\r\nThe Skylake X family in particular will feature five SKUs that include a 12 core, 10 core, 8 core and a 6 core model. The 10, 8 and 6 core models will be based on the Skylake architecture. All Skylake-X chips will feature a rated TDP of 140W and we can expect clock speeds north of the 3.0 GHz range and boosts of 4.0 GHz+ on the 6 core and 8 core chips. The 12 core chip will be the new flagship and will come with more PCIe lanes with faster memory support.\r\n\r\nIntel X299\r\n\r\nIntel’s new X299 chipset will be the latest PCH to support the enthusiast processors. The X299 platform will be centered around the LGA 2066 socket which will be compatible with at least two generations of processors. In specs, the X299 chipset offers up to 24 PCIe Gen 3.0 lanes. The chip also offers up to quad channel memory with speeds up to DDR4-2667 MHz (native). Kaby Lake X series processors will only support dual channel RAM but will stick to the native speeds of 2667 MHz.\r\n\r\nThe Basin Falls PCH also offers 10 USB 3.0, 8 USB 2.0 ports, SATA 3.0, and Intel LAN (Jacksonville PHY) controllers. Additional features include Enhanced SPI, SPI, LPC, SMBus and HD audio which are integrated underneath its hood.\r\n\r\nThe Skylake X and Kaby Lake X are meant to make a formal debut on 30th May followed by market availability by the end of June. This marks the announcement at PC Gaming Show which is in line with what we have recently been hearing so expect to see lots of motherboards being shown off at the event floor\r\n\r\n', 'hardware', NULL, NULL, 0, 0, 'images/5.png');
INSERT INTO `topic` (`topic_id`, `user_id`, `total_views`, `title`, `posted`, `content`, `category`, `category2`, `category3`, `deleted`, `star`, `picture`) VALUES
(6, 7, 0, 'NVIDIA is Not Even The Least Bit Worried About AMD’s Radeon RX Vega – CEO Jen-Hsun Huang Expects The Competitive Position To Remain Unchanged', '2017-05-13 17:11:17', 'NVIDIA’s CEO Confident With Pascal GPUs, Doesn’t Expect Competitive Position To Change With RX Vega Cards From Rival\r\n\r\nDuring the Q/A session, Barclays Capital analyst, Blayne Curtis, asked NVIDIA’s CEO on how he sees the competitive landscape change in the second half of 2017. NVIDIA has already shown some big guns in first half of 2017 such as the GTX 1080 Ti, NVIDIA Titan Xp and the updated GTX 1080 cards. These cards face no competition from AMD as of now and offer brilliant performance to gamers, enthusiasts and even professional users (in the case of the NVIDIA Titan Xp).\r\n\r\n	\r\nGTC 2017  ?  Hardware  ?  Report\r\nNVIDIA is Not Even The Least Bit Worried About AMD’s Radeon RX Vega – CEO Jen-Hsun Huang Expects The Competitive Position To Remain Unchanged\r\nAuthor Photo\r\nBy Hassan Mujtaba\r\nMay 10\r\n239Shares\r\nShare Tweet Submit\r\n\r\nLast night, NVIDIA posted another successful financial quarter and posted a revenue of $1.93 Billion, a 48 percent increase from last year. During the earnings call, NVIDIA’s CEO, Jen-Hsun Huang, being confident, revealed that he doesn’t expect the competitive position to change in 2017 on the gaming GPU side.\r\nNVIDIA’s CEO Confident With Pascal GPUs, Doesn’t Expect Competitive Position To Change With RX Vega Cards From Rival\r\n\r\nDuring the Q/A session, Barclays Capital analyst, Blayne Curtis, asked NVIDIA’s CEO on how he sees the competitive landscape change in the second half of 2017. NVIDIA has already shown some big guns in first half of 2017 such as the GTX 1080 Ti, NVIDIA Titan Xp and the updated GTX 1080 cards. These cards face no competition from AMD as of now and offer brilliant performance to gamers, enthusiasts and even professional users (in the case of the NVIDIA Titan Xp).\r\nRelated\r\nAMD Announces Computex Press Conference For 31st May – Radeon RX Vega, X399 HEDT Platform and More To Be Expected\r\n\r\nNVIDIA’s newly built HQ is designed as a multi-layered Polygon. It costs 370 million dollars and has a total area of 46,500 square feet, housing 2500 employees.\r\n\r\nJen-Hsun confidently replied to the question, saying that he doesn’t expects the competitive position to change even in the second half of 2017. Following is the full reply (DVHardware via SeekingAlpha):\r\n\r\n    Blayne Curtis – Barclays Capital, Inc.\r\n    Thanks. And then just moving to the gaming GPU side, I was just wondering if you can just talk about the competitive landscape looking back at the last refresh. And then looking forward into the back half of this year, I think your competitors have a new platform. I’m just curious as to your thoughts as to how the share worked out on the previous refresh and then the competitiveness into the second half of this year.\r\n\r\n    Jen-Hsun Huang – NVIDIA Corp.\r\n    My assessment is that the competitive position is not going to change.\r\n\r\nBy competitors having a new platform, I guess Blayne is suggesting the launch of AMD’s upcoming Radeon RX Vega series of cards which are expected to arrive during mid 2017. Jen-Hsun’s reply to the analyst was in a very confident tone and reveals that the GPU giant faces no threat from a competitor launch. However, NVIDIA’s CEO stated some time ago that they respect the capabilities of their graphics competitors.\r\n\r\n	\r\nGTC 2017  ?  Hardware  ?  Report\r\nNVIDIA is Not Even The Least Bit Worried About AMD’s Radeon RX Vega – CEO Jen-Hsun Huang Expects The Competitive Position To Remain Unchanged\r\nAuthor Photo\r\nBy Hassan Mujtaba\r\nMay 10\r\n239Shares\r\nShare Tweet Submit\r\n\r\nLast night, NVIDIA posted another successful financial quarter and posted a revenue of $1.93 Billion, a 48 percent increase from last year. During the earnings call, NVIDIA’s CEO, Jen-Hsun Huang, being confident, revealed that he doesn’t expect the competitive position to change in 2017 on the gaming GPU side.\r\nNVIDIA’s CEO Confident With Pascal GPUs, Doesn’t Expect Competitive Position To Change With RX Vega Cards From Rival\r\n\r\nDuring the Q/A session, Barclays Capital analyst, Blayne Curtis, asked NVIDIA’s CEO on how he sees the competitive landscape change in the second half of 2017. NVIDIA has already shown some big guns in first half of 2017 such as the GTX 1080 Ti, NVIDIA Titan Xp and the updated GTX 1080 cards. These cards face no competition from AMD as of now and offer brilliant performance to gamers, enthusiasts and even professional users (in the case of the NVIDIA Titan Xp).\r\nRelated\r\nAMD Announces Computex Press Conference For 31st May – Radeon RX Vega, X399 HEDT Platform and More To Be Expected\r\n\r\nNVIDIA’s newly built HQ is designed as a multi-layered Polygon. It costs 370 million dollars and has a total area of 46,500 square feet, housing 2500 employees.\r\n\r\nJen-Hsun confidently replied to the question, saying that he doesn’t expects the competitive position to change even in the second half of 2017. Following is the full reply (DVHardware via SeekingAlpha):\r\n\r\n    Blayne Curtis – Barclays Capital, Inc.\r\n    Thanks. And then just moving to the gaming GPU side, I was just wondering if you can just talk about the competitive landscape looking back at the last refresh. And then looking forward into the back half of this year, I think your competitors have a new platform. I’m just curious as to your thoughts as to how the share worked out on the previous refresh and then the competitiveness into the second half of this year.\r\n\r\n    Jen-Hsun Huang – NVIDIA Corp.\r\n    My assessment is that the competitive position is not going to change.\r\n\r\nBy competitors having a new platform, I guess Blayne is suggesting the launch of AMD’s upcoming Radeon RX Vega series of cards which are expected to arrive during mid 2017. Jen-Hsun’s reply to the analyst was in a very confident tone and reveals that the GPU giant faces no threat from a competitor launch. However, NVIDIA’s CEO stated some time ago that they respect the capabilities of their graphics competitors.\r\nThe statement from Jen-Hsun confirms that NVIDIA is feeling very dominant as of right now with a vast array of enthusiast and high-end graphics cards based on the Pascal GPU architecture. In just a few hours, NVIDIA will be unveiling their next computer powerhouse, the Volta based GV100 at GTC17 and the next-generation GDDR6 memory has already been showcased yesterday which would make its way on the Volta based high-end gaming graphics cards, early next year.', 'hardware', NULL, NULL, 0, 0, 'images/6.png'),
(7, 7, 0, 'Google''s Deep Mind Gives AI a Memory Boost That Lets It Navigate London''s Underground', '2017-05-13 17:11:58', '\r\n\r\nGoogle’s DeepMind artificial intelligence lab does more than just develop computer programs capable of beating the world’s best human players in the ancient game of Go. The DeepMind unit has also been working on the next generation of deep learning software that combines the ability to recognize data patterns with the memory required to decipher more complex relationships within the data.\r\n\r\nDeep learning is the latest buzz word for artificial intelligence algorithms called neural networks that can learn over time by filtering huge amounts of relevant data through many “deep” layers. The brain-inspired neural network layers consist of nodes (also known as neurons). Tech giants such as Google, Facebook, Amazon, and Microsoft have been training neural networks to learn how to better handle tasks such as recognizing images of dogs or making better Chinese-to-English translations. These AI capabilities have already benefited millions of people using Google Translate and other online services.\r\n\r\nBut neural networks face huge challenges when they try to rely solely on pattern recognition without having the external memory to store and retrieve information. To improve deep learning’s capabilities, Google DeepMind created a “differentiable neural computer” (DNC) that gives neural networks an external memory for storing information for later use.\r\n\r\n“Neural networks are like the human brain; we humans cannot assimilate massive amounts of data and we must rely on external read-write memory all the time,” says Jay McClelland, director of the Center for Mind, Brain and Computation at Stanford University. “We once relied on our physical address books and Rolodexes; now of course we rely on the read-write storage capabilities of regular computers.”\r\n\r\nMcClelland is a cognitive scientist who served as one of several independent peer reviewers for the Google DeepMind paper that describes development of this improved deep learning system. The full paper is presented in the 12 Oct 2016 issue of the journal Nature.\r\n\r\nThe DeepMind team found that the DNC system’s combination of the neural network and external memory did much better than a neural network alone in tackling the complex relationships between data points in so-called “graph tasks.” For example, they asked their system to either simply take any path between points A and B or to find the shortest travel routes based on a symbolic map of the London Underground subway.\r\n\r\nAn unaided neural network could not even finish the first level of training, based on traveling between two subway stations without trying to find the shortest route. It achieved an average accuracy of just 37 percent after going through almost two million training examples. By comparison, the neural network with access to external memory in the DNC system successfully completed the entire training curriculum and reached an average of 98.8 percent accuracy on the final lesson.\r\n\r\nThe external memory of the DNC system also proved critical to success in performing logical planning tasks such as solving simple block puzzle challenges. Again, a neural network by itself could not even finish the first lesson of the training curriculum for the block puzzle challenge. The DNC system was able to use its memory to store information about the challenge’s goals and to effectively plan ahead by writing its decisions to memory before acting upon them.\r\n\r\nIn 2014, DeepMind’s researchers developed another system, called the neural Turing machine, that also combined neural networks with external memory. But the neural Turing machine was limited in the way it could access “memories” (information) because such memories were effectively stored and retrieved in fixed blocks or arrays. The latest DNC system can access memories in any arbitrary location, McClelland explains.\r\n\r\nThe DNC system’s memory architecture even bears a certain resemblance to how the hippocampus region of the brain supports new brain cell growth and new connections in order to store new memories. Just as the DNC system uses the equivalent of time stamps to organize the storage and retrieval of memories, human “free recall” experiments have shown that people are more likely to recall certain items in the same order as first presented.\r\n\r\nDespite these similarities, the DNC’s design was driven by computational considerations rather than taking direct inspiration from biological brains, DeepMind’s researchers write in their paper. But McClelland says that he prefers not to think of the similarities as being purely coincidental.\r\n\r\n“The design decisions that motivated the architects of the DNC were the same as those that structured the  human memory system, although the latter (in my opinion) was designed by a gradual evolutionary process, rather than by a group of brilliant AI researchers,” McClelland says.\r\n\r\nHuman brains still have significant advantages over any brain-inspired deep learning software. For example, human memory seems much better at storing information so that it is accessible by both context or content, McClelland says. He expressed hope that future deep learning and AI research could better capture the memory advantages of biological brains.\r\n\r\nDeepMind’s DNC system and similar neural learning systems may represent crucial steps for the ongoing development of AI. But the DNC system still falls well short of what McClelland considers the most important parts of human intelligence.\r\n\r\n    The DNC is a sophisticated form of external memory, but ultimately it is like the papyrus on which Euclid wrote the elements. The insights of mathematicians that Euclid codified relied (in my view) on a gradual learning process that structured the neural circuits in their brains so that they came to be able to see relationships that others had not seen, and that structured the neural circuits in Euclid’s brain so that he could formulate what to write.  We have a long way to go before we understand fully the algorithms the human brain uses to support these processes. \r\n\r\nIt’s unclear when or how Google might take advantage of the capabilities offered by the DNC system to boost its commercial products and services. The DeepMind team was “heads down in research” or too busy with travel to entertain media questions at this time, according to a Google spokesperson.\r\n\r\nBut Herbert Jaeger, professor for computational science at Jacobs University Bremen in Germany, sees the DeepMind team’s work as a “passing snapshot in a fast evolution sequence of novel neural learning architectures.” In fact, he’s confident that the DeepMind team already has something better than the DNC system described in the Nature paper. (Keep in mind that the paper was submitted back in January 2016.)\r\n\r\nDeepMind’s work is also part of a bigger trend in deep learning, Jaeger says. The leading deep learning teams at Google and other companies are racing to build new AI architectures with many different functional modules—among them, attentional control or working memory; they then train the systems through deep learning. \r\n\r\n“The DNC is just one among dozens of novel, highly potent, and cleverly-thought-out neural learning systems that are popping up all over the place,” Jaeger says.\r\n', 'hardware', NULL, NULL, 0, 1, 'images/7.png'),
(8, 7, 0, 'Nolan Bushnell Says His New Virtual Reality Startup Has the Keys to the Holodeck—and it’s Portable', '2017-05-13 17:12:37', 'Modal VR''s virtual reality system for commercial applications includes a wearable headset, a full-body motion tracking suit, and a computer peripheral it calls the VR Fabricator.\r\n\r\nI’ve been talking to a lot of academics, investors, and analysts lately to get a sense of what’s coming down the path in virtual and augmented reality.\r\n\r\nAnd at least for VR—expensive, immersive, full on virtual reality—they’ve been telling me to look to gear for the business world, not the consumer world, for the next big thing. That’s because VR hardware won’t be cheap enough, at least in an untethered form, for the average consumer anytime soon.\r\n\r\nSo maybe Modal VR, a startup company that came out of stealth today that is building VR hardware exclusively for business applications, will hit the sweet spot for the technology in 2017. Cofounder Nolan Bushnell, founder of Atari and Chuck E. Cheese, thinks so. “My past successes have always been by being at the right time, at the right place,” he said in a video statement released at the launch.\r\n\r\nThe company says it will be shipping multi-user, wireless, VR systems to developers soon, but didn’t disclose a date.\r\n\r\n“For those of us who grew up on “Star Trek,” the holodeck has always been the gold standard, Bushnell said in the launch video. “Modal VR is the first time that I believe we actually have the holodeck.”\r\n\r\nThe portable system, the company says, consists of a computer peripheral it’s calling the VR Fabricator, a virtual reality visor, a full-body motion tracking suit, and software; users will have to bring their own Mac or PC. Each Fabricator can support up to 10 users wearing visors and tracking suits; the peripherals can also be networked to add users or expand the range behind the initial 83,000 square meters.\r\n\r\nYou know this is a Bushnell effort because the featured app in the launch video is a gaming app, a virtual battle played out on what is, in the real world, a soccer field. It’s not a bad idea—I can easily see how a company makes a birthday party business out of bringing VR games to kids, competing with permanent laser tag venues, without requiring the purchase of real estate.\r\n\r\nOther apps suggested on the website include real estate, allowing walkthroughs of multiple homes from a realtor’s office, perhaps; emergency response training; immersive exhibits in museums; and virtual field trips and simulations for students. “I want to have students be able to walk through the human body, to walk amongst the planets,” says Bushnell.\r\n', 'hardware', NULL, NULL, 0, 0, 'images/8.png'),
(9, 7, 0, 'Deep Learning Startup Maluuba''s AI Wants to Talk to You', '2017-05-13 17:13:18', 'Apple’s personal assistant Siri is more of a glorified voice recognition feature of your iPhone than a deep conversation partner. A personal assistant that could truly understand human conversations and written texts might actually represent an artificial intelligence capable of matching or exceeding human intelligence. The Canadian startup Maluuba hopes to help the tech industry achieve such a breakthrough by training AI to become better at understanding languages. The key, according Maluuba’s leaders, is building a better way to train AIs.\r\n\r\nLike humans, AI can only get better at understanding languages by practicing. Maluuba aims to use the popular AI technique known as deep learning to improve computer systems’ language skills in key areas such as reading comprehension and having conversations. Toward that end, Maluuba has released two new sets of data designed to train deep-learning algorithms on becoming better at those crucial language skills.\r\n\r\n“If you teach a machine to truly understand language, you’ve truly built artificial intelligence,” says Mo Musbah, vice president of products at Maluuba. “We’re excited about teaching a machine to truly engage in conversation or language comprehension.”\r\n\r\nBig tech companies such as Google and Microsoft already use machine learning algorithms to help automatically perform language translation. For example, the popular Google Translate service now uses deep-learning algorithms to help Google users more accurately translate written sentences from Chinese to English or vice versa. But even Google Translate still has problems translating some sentences because its underlying AI lacks the the needed language comprehension skills.\r\n\r\nThe reality is that today’s AI technology is still a far cry from having the natural language skills of robots and computers depicted in science fiction films. The typical question-and-answer interactions with Apple’s Siri pale in comparison with the natural dialogue that flows between actor Joaquin Phoenix’s character and the AI named Samantha voiced by actress Scarlett Johansson in the 2013 film “Her.” Phoenix’s character eventually forms a romantic relationship with his AI companion as they share meaningful conversations that include both moments of laughter and sorrow.\r\n\r\nMusbah, the vice president of product at Maluuba, brought up Samantha as an example of an AI possessing language skills far beyond today’s computer systems.\r\n\r\n    To get to the point where you can get Samantha from “Her,” you need to get the fundamental blocks of understanding language. She reads through emails and processes and provides back-and-forth dialogue. We’re excited because these are the stepping stones to get to point where you have true AI.\r\n\r\nDeep-learning algorithms have the power to help AI learn on its own over time by filtering huge amounts of relevant data. In the case of fundamental language skills, that means deep-learning researchers need huge amounts of data that can challenge an AI to perform certain conversational tasks or comprehension and reasoning tasks. Creating those datasets takes both time and effort.\r\n\r\n“The big challenge with deep learning in our space is that because it’s so data driven, the models you end up training are only as complex as the data you train them on,” says Adam Trischler, a research scientist at Maluuba.\r\n\r\nTech giants such as Google’s DeepMind AI lab and Facebook AI Research created the first big, publicly-available datasets for machine comprehension that contained enough data to train deep-learning algorithms. DeepMind’s CNN dataset creates comprehension challenges by deleting words from certain sections of CNN news articles to create “fill-in-the-blank” questions. Facebook AI Research created a similarly large dataset by deleting certain words from the passages of children’s books.\r\n\r\nDeepMind’s and Facebook AI Research’s datasets were important first steps in training deep-learning algorithms, Trischler says. But he explains that these “fill-in-the-blank” questions can often be solved through simple methods such as context or synonym matching, rather than really challenging an AI’s language comprehension and reasoning.\r\n\r\nSo Maluuba set out to build a better dataset. It has now released the result, the “NewsQA” dataset, with more than 110,000 training questions. To build it, the startup enlisted the help of human workers through an online crowdsourcing service similar to Amazon’s Mechanical Turk. One set of workers looked at the highlights from CNN news articles and tried to come up with challenging comprehension questions. A second set of workers tried to answer those questions. And a third set of workers helped validate the pairs of questions and answers.\r\n\r\n“We found that a large majority of the questions in our dataset do require reasoning beyond the context matching and synonym matching in previous datasets,” Trischler says. “That was our goal and we achieved that.”\r\n\r\nMaluuba has also released a second “Frames” dataset with 1,368 dialogues to help train deep-learning algorithms on conversations. But instead of using an online crowd of anonymous workers to create the dataset, the startup invited 12 human volunteers to its Montreal-based lab. There the volunteers engaged in online chat conversations where one person pretended to be a customer looking to book a vacation and the second person pretended to be a travel agent consulting a database with information on different hotels, flights and vacation destinations.\r\n\r\nThese human-to-human conversations showed Maluuba that people frequently went back-and-forth on different travel routes and vacation possibilities. Examples of such dialogue challenge AI by requiring the computer systems to retain a memory of the different possibilities as a basis for comparison.\r\n\r\nSuch conversational capability remains far beyond Apple’s Siri or any online chatbots. Those can only answer questions about individual or sequential pieces of information that come in a specific order, says Layla El Asri, a research scientist at Maluuba. Previously, the most challenging dialogue dataset that was publicly available for deep-learning researchers was designed for a sequential process of searching for a restaurant with specific steps such as type of food, then budget, then  geographic location.\r\n\r\nBy comparison, Maluuba’s new publicly-available Frames dataset challenges deep-learning algorithms to have the memory to hold a natural conversation that can go back-and-forth on different points such as hotels, flights, and vacation destinations without necessarily following a specific order. The Frames dataset also allows researchers to study other aspects of natural language that still pose a huge challenge for deep-learning AI.\r\n\r\n“The human beings did a lot of summarizing of information in the database, such as ‘The cheapest package I have is this one’ or ‘I don’t have anything under $2,000,’” El Asri says. “There is no natural-dialogue generation model that can do that kind of summarization.”\r\n\r\nThe Canadian startup has already begun using its datasets to begin training its own deep-learning algorithms to become better at both natural language comprehension and dialogue. But it has also made its new datasets publicly available to other researchers in the hopes of boosting the state of machine comprehension technology across the industry. The public release of such datasets could also raise Maluuba’s prestige if such datasets become the new industry benchmarks for testing deep-learning algorithms’ performance.\r\n\r\nMaluuba’s bet on language as the key to elevating AI could also eventually face its own kind of test. The startup is working with a researcher at McGill University in Montreal on training an AI system that could take on the Winograd Schema Challenge: A test designed to determine how well an AI system can handle commonsense reasoning. One classic example of a Winograd Schema Challenge question goes: “I tried to put my computer inside the briefcase, but it was too small.” The AI system would have to figure out whether “it was too small” refers to the briefcase or the computer.\r\n\r\n“The Winograd Schema Challenge is all about common sense,” Trischler says. “The reason we see that as something very important is because that goes hand-in-hand with the machine comprehension we’re working on.”\r\n\r\nIf Maluuba is right, training AI to become better at language comprehension and conversation could do much more than just deliver a more helpful Siri or smarter online chatbots. We might someday see an intelligent robot such as C-3PO or computing system such as Samantha step out of science fiction into reality.', 'hardware', NULL, NULL, 0, 0, 'images/9.png');

-- --------------------------------------------------------

--
-- Table structure for table `user`
--

CREATE TABLE `user` (
  `username` varchar(16) NOT NULL,
  `password` varchar(32) NOT NULL,
  `id_num` bigint(20) UNSIGNED NOT NULL,
  `first_name` varchar(32) NOT NULL,
  `mid_name` varchar(32) DEFAULT NULL,
  `last_name` varchar(32) NOT NULL,
  `time_reg` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `birthday` date NOT NULL,
  `email` varchar(50) NOT NULL,
  `bio` text,
  `block_sts` tinyint(1) UNSIGNED NOT NULL DEFAULT '0',
  `user_rank` tinyint(1) UNSIGNED DEFAULT '1',
  `reputation` bigint(32) UNSIGNED NOT NULL DEFAULT '0',
  `voters` bigint(32) UNSIGNED NOT NULL DEFAULT '0',
  `votetoday` int(1) UNSIGNED NOT NULL DEFAULT '0',
  `user_pict` varchar(255) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

--
-- Dumping data for table `user`
--

INSERT INTO `user` (`username`, `password`, `id_num`, `first_name`, `mid_name`, `last_name`, `time_reg`, `birthday`, `email`, `bio`, `block_sts`, `user_rank`, `reputation`, `voters`, `votetoday`, `user_pict`) VALUES
('admin', '0c7540eb7e65b553ec1ba6b20de79608', 0, 'admin', 'admin', 'admin', '2017-05-09 04:42:00', '0000-00-00', 'admin@gmail.com', 'admin', 0, 1, 0, 0, 0, 'img/5.jpg'),
('chen_vie', '7b4fe7e7bab2888b56094a338dcb88f3', 7, 'Vievin', '', 'Efendy', '2017-05-11 13:32:33', '0000-00-00', 'vievin.efendy@ti.ukdw.ac.id', NULL, 0, 1, 0, 0, 0, ''),
('lucidplayer', '4ae888fa0c4fdfab26ff3516c2af4a5e', 1, 'Nathaniel', NULL, 'Clarence', '2017-04-11 02:34:17', '1997-01-13', 'nathaniel.clarence@ti.ukdw.ac.id', NULL, 0, 3, 0, 0, 0, ''),
('maria', '12312ea631a2a5d7d259ba238acb9a90', 2, 'maria', 'theresa', 'rahardjo', '2017-05-09 04:45:26', '1997-04-29', 'maria.theresa@gmail.com', NULL, 0, 1, 0, 0, 0, '');

--
-- Indexes for dumped tables
--

--
-- Indexes for table `attachment`
--
ALTER TABLE `attachment`
  ADD PRIMARY KEY (`attach_id`);

--
-- Indexes for table `report`
--
ALTER TABLE `report`
  ADD PRIMARY KEY (`report_id`);

--
-- Indexes for table `topic`
--
ALTER TABLE `topic`
  ADD UNIQUE KEY `topic_id` (`topic_id`);

--
-- Indexes for table `user`
--
ALTER TABLE `user`
  ADD PRIMARY KEY (`username`,`email`),
  ADD UNIQUE KEY `num_id` (`id_num`);

--
-- AUTO_INCREMENT for dumped tables
--

--
-- AUTO_INCREMENT for table `attachment`
--
ALTER TABLE `attachment`
  MODIFY `attach_id` bigint(100) NOT NULL AUTO_INCREMENT;
--
-- AUTO_INCREMENT for table `report`
--
ALTER TABLE `report`
  MODIFY `report_id` bigint(20) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=3;
--
-- AUTO_INCREMENT for table `topic`
--
ALTER TABLE `topic`
  MODIFY `topic_id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=10;
--
-- AUTO_INCREMENT for table `user`
--
ALTER TABLE `user`
  MODIFY `id_num` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=8;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
